{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Customer Support Analysis\n",
    "\n",
    "Written by Domas Budrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "column_tweet_id = []\n",
    "column_author_id = []\n",
    "column_inboud = []\n",
    "column_created_at = []\n",
    "column_text = []\n",
    "column_response_tweet_id = []\n",
    "column_in_response_to_tweet_id = []\n",
    "\n",
    "\n",
    "\n",
    "with open ('twcs.csv', encoding='utf-8') as csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        \n",
    "        column_tweet_id.append(row['tweet_id'])\n",
    "        column_text.append(row['text'])\n",
    "      \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter ID and Text is being split into separate list. Because of this, we are able to sort and manipulate data using separate lists and not the entire file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets is:  2811774\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "count_rows = sum(1 for line in column_tweet_id)\n",
    "print ('Number of Tweets is: ', count_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "We are assigning number of tweets to the variable **count_rows**.\n",
    "Then,   `sum(1 for line in column_tweet_id)`   is used to assign number \"1\" to every row in **column_tweet_id** and with the sum() function we are able get the total of all number \"1\"s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ID :   269\n",
      "@115770 こんにちは、アマゾン公式です。Fire TV Stickが見れないというのは、どのような状況でしょうか。一般的なトラブルシューティングを記載したヘルプがございますので、ご参照ください。https://t.co/2pbG55qJ7h ET\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "tweet_269_index = column_tweet_id.index('269')\n",
    "text = column_text[tweet_269_index]\n",
    "\n",
    "print(\"Tweet ID :  \", column_tweet_id[tweet_269_index])\n",
    "print(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Because earlier we stored all the values of tweet_id into list *column_tweet_id* now we are able to search this list for specific value. \n",
    "By using `column_tweet_id.index('269')` we are able to specify that we are searching for tweet_id=269 and the index of it will be stored to the variable **tweet_269_index **.\n",
    "Then, we are able to use this index variable to search for a text related to tweet_id=269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets that contains less than 50% of English characters is: 20025\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "ascii_char_set = set('0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c')\n",
    "\n",
    "less_than = 0                 \n",
    "                   \n",
    "for i in column_text:\n",
    "    \n",
    "    text_len = len(i)  \n",
    "    ascii_count = sum(char in ascii_char_set for char in i)        \n",
    "     \n",
    "\n",
    "    if ((ascii_count / text_len ) < 0.5):\n",
    "        less_than +=1\n",
    "    \n",
    "\n",
    "print ('The number of tweets that contains less than 50% of English ' \n",
    "       'characters is:',less_than)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "In Question 3 we are using variable **ascii_char_set** to store the set of characters that represents ASCII. \n",
    "Then, using `for i in column_text:` we are able to read every line from **column_text** and store it in variable **i**. After that, we are assigning the length of **i** to **text_len**.\n",
    "Now, in **ascii_count** we are able to store the sum of all characters of **i** that match any value in **ascii_char_set**. \n",
    "Finally, we make an if statement to compare if average value is less than 0.5 (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique twitter names :  716567\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "import re\n",
    "\n",
    "name_list=[]\n",
    "\n",
    "for i in column_text:\n",
    "\n",
    "    name_list += re.findall(\"[@]\\w+\", i)\n",
    "    \n",
    "unique_names = list(set(name_list))\n",
    "\n",
    "print('Number of unique twitter names : ',len(unique_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "First, we must import regular expression library 're'. Then, we create an empty list **name_list** to store future name values. Then, in for loop we use re function *findall* where `[@]\\w+` means that every word starting character @ and ending with any other value `\\w+` will be stored to the **name_list**. Furthermore, `unique_names = list(set(name_list))` is used to store unordered unique values of **name_list**. Finally, we are able to get the count of unique twitter names by using *len()* fuction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 of the most used twitter names and their count :\n",
      "('@AmazonHelp', 136815)\n",
      "('@AppleSupport', 98024)\n",
      "('@AmericanAir', 50507)\n",
      "('@Uber_Support', 47226)\n",
      "('@Delta', 42559)\n",
      "('@115858', 40726)\n",
      "('@VirginTrains', 37592)\n",
      "('@SouthwestAir', 34375)\n",
      "('@Tesco', 34087)\n",
      "('@SpotifyCares', 31214)\n"
     ]
    }
   ],
   "source": [
    "#Question 5\n",
    "import collections\n",
    "print ('10 of the most used twitter names and their count :')\n",
    "for i in collections.Counter(name_list).most_common(10):\n",
    "    print (i)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "*collections* must be imported at first. Then with `collections.Counter(name_list)` we are able get the count of every twitter name used in this data file. Next, we can use one of the built in functions for Counter and get the value of 10 `most_common` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique twitter hashtags(#) :  241942\n"
     ]
    }
   ],
   "source": [
    "#Question 6\n",
    "    \n",
    "hashtag_list=[]\n",
    "\n",
    "for i in column_text:\n",
    "\n",
    "    hashtag_list += re.findall(\"[#]\\w+\", i)\n",
    "\n",
    "unique_hashtag = list(set(hashtag_list))\n",
    "\n",
    "print('Number of unique twitter hashtags(#) : ', len(hashtag_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "For this question we will be using 're' library again. We create an empty list **hashtag_list** to store future hashtag values. Then, in for loop we use re function *findall()* where `[#]\\w+` means that every word starting character # and ending with any other value \\w+ will be stored to the hashtag_list, because of this a single # will not be added to the list. Furthermore, `unique_hashtag = list(set(hashtag_list))` is used to store unordered unique values of hashtag_list. Finally, we are able to get the count of unique twitter names by using len() fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 of the most used twitter hashtags and their count :\n",
      "('#mobile_Care', 2151)\n",
      "('#AATeam', 1621)\n",
      "('#fail', 1604)\n",
      "('#Amazon', 1385)\n",
      "('#iPhoneX', 1247)\n",
      "('#iOS11', 1226)\n",
      "('#hppsdr', 1208)\n",
      "('#help', 1116)\n",
      "('#mobile_CareXI', 1050)\n",
      "('#CustomerService', 1030)\n"
     ]
    }
   ],
   "source": [
    "#Question 7\n",
    "print ('10 of the most used twitter hashtags and their count :')\n",
    "for i in collections.Counter(hashtag_list).most_common(10):\n",
    "    print (i)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Using with collections.Counter(name_list) we are able get the count of every hashtag used in this data file. Next, we can use one of the built in functions for Counter and get the value of 10 most_common values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('to', 1679423)\n",
      "('the', 1408351)\n",
      "('you', 1264804)\n",
      "('I', 1154133)\n",
      "('t', 1020842)\n",
      "('a', 885713)\n",
      "('and', 801336)\n",
      "('for', 754446)\n",
      "('your', 683002)\n",
      "('co', 655927)\n",
      "('https', 654523)\n",
      "('this', 532376)\n",
      "('it', 497247)\n",
      "('on', 485821)\n",
      "('is', 482405)\n",
      "('can', 474957)\n",
      "('in', 467208)\n",
      "('us', 445446)\n",
      "('with', 443049)\n",
      "('We', 429121)\n"
     ]
    }
   ],
   "source": [
    "#Question 8\n",
    "\n",
    "all_words = []\n",
    "for i in column_text:\n",
    "    \n",
    "    all_words += re.findall(r'[a-zA-Z]+', i )\n",
    "    \n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#stop_words = stopwords.words('english')\n",
    "\n",
    "#for w in list(all_words):  # iterating on a copy since removing will mess things up\n",
    "#    if w in stop_words:\n",
    "#        all_words.remove(w)\n",
    "\n",
    "for i in collections.Counter(all_words).most_common(20):\n",
    "    print (i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "With the first for loop we are able to find only the words that start with English letters (ignoring lowercase or uppercase) and assign to the list **all_words**\n",
    "Then, we are using the Counter() to display 20 most common words: \n",
    "`for i in collections.Counter(all_words).most_common(20)`\n",
    "\n",
    "Commented code should be used to remove stop words from the list of **all_words**. I was not able to see the result of this code because my computer was taking way too long to execute it. \n",
    "\n",
    "    nltk.download(\"stopwords\")\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for w in list(all_words):  # iterating on a copy since removing will mess things up\n",
    "        if w in stop_words:\n",
    "            all_words.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
